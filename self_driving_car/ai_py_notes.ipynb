{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ai.py notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the architecture of the Neural Network\n",
    "\n",
    "* References\n",
    "    - http://pytorch.org/docs/master/nn.html#containers\n",
    "    - http://pytorch.org/docs/master/nn.html#linear-layers\n",
    "    - http://pytorch.org/docs/master/nn.html#torch.nn.functional.relu\n",
    "    - http://pytorch.org/docs/master/nn.html#non-linear-activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        '''\n",
    "        Creates architecture of our neural network class\n",
    "        \n",
    "        input_size = size of input vector\n",
    "        nb_action = number of actions\n",
    "        '''\n",
    "        # as defined here http://pytorch.org/docs/master/nn.html#containers\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        \n",
    "        # creating a linear layer as defined here http://pytorch.org/docs/master/nn.html#linear-layers\n",
    "        self.fc1 = nn.Linear(in_features=self.input_size, out_features=30, bias=True)\n",
    "        \n",
    "        # creating a liner layer as defined here http://pytorch.org/docs/master/nn.html#linear-layers\n",
    "        self.fc2 = nn.Linear(in_features=30, out_features=nb_action, bias=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Applies forward propogation to neural network and returns the Q-values\n",
    "        \n",
    "        state = input state for the neural network (a torch Variable)\n",
    "        '''\n",
    "        \n",
    "        # the statement passes the input state values to first linear layer (input layer) and applies,\n",
    "        # rectifier linear unit function as defined here http://pytorch.org/docs/master/nn.html#non-linear-activations\n",
    "        # returns the hidden neuron values\n",
    "        x = F.relu(self.fc1(state))\n",
    "        \n",
    "        # the statment passes the hidden values to second linear layer(output layer)\n",
    "        # and obtain the output q_values\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Implementing Experience Replay\n",
    "\n",
    "* References\n",
    "    - https://www.programiz.com/python-programming/methods/built-in/zip\n",
    "    - http://pytorch.org/docs/master/torch.html?indexing-slicing-joining-mutating-ops#indexing-slicing-joining-mutating-ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        '''\n",
    "        Initilizes the ReplayMemory (a.k.a. ExperienceMemory) Class \n",
    "        \n",
    "        capacity = length of replay memory\n",
    "        '''\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        \n",
    "    def push(self, event):\n",
    "        '''\n",
    "        Appends event to replay memory and ensures memory does not exceeds given capacity\n",
    "        \n",
    "        event = event tuple (state, state_prime, reward, action)\n",
    "        '''\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Fetches a random sample from replay memory, reshapes it and returns as a torch Variable\n",
    "        \n",
    "        batch_size = number of random samples to fetch\n",
    "        '''\n",
    "        # as defined here https://www.programiz.com/python-programming/methods/built-in/zip\n",
    "        # if list = [(1,2,3),(4,5,6)], then zip(*list) = [(1,4),(2,3),(5,6)]\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        # as defined here http://pytorch.org/docs/master/torch.html?indexing-slicing-joining-mutating-ops#indexing-slicing-joining-mutating-ops\n",
    "        return map(lambda var: Variable(torch.cat(seq=x, dim=0)), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip function demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (7, 8, 9), (16, 17, 18), (4, 5, 6)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_memory = [(1,2,3), (4,5,6), (7,8,9), (10,11,12), (13,14,15), (16,17,18)]\n",
    "example_batch = random.sample(example_memory, 4)\n",
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 7, 16, 4), (2, 8, 17, 5), (3, 9, 18, 6)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = zip(*example_batch)\n",
    "list(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network parameters demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       "  0.2680  0.1040  0.1754  0.0519  0.4036\n",
       " -0.2909 -0.3661  0.0451  0.0694 -0.1678\n",
       " -0.1794  0.4193 -0.3143  0.4197  0.0522\n",
       "  0.0097 -0.1910 -0.0391  0.3963 -0.2185\n",
       " -0.3662 -0.2149 -0.1219  0.2423 -0.0812\n",
       " -0.3110  0.2229 -0.2322  0.2683 -0.4236\n",
       " -0.3746  0.3692  0.1135  0.4237  0.3748\n",
       "  0.4119 -0.4135  0.3867 -0.2454 -0.0706\n",
       "  0.2212 -0.1920 -0.0525  0.4148 -0.0283\n",
       " -0.1114 -0.4027  0.0401  0.0896 -0.3335\n",
       " -0.2436 -0.2807  0.3331  0.4094 -0.2917\n",
       "  0.2656  0.1724  0.0951  0.3376 -0.1951\n",
       "  0.2393 -0.2010  0.0043  0.1937  0.2133\n",
       "  0.3713 -0.1818 -0.1340  0.4250  0.0696\n",
       " -0.2187 -0.1999  0.2647  0.2645  0.0349\n",
       " -0.2254 -0.0123  0.1156 -0.0751  0.3148\n",
       "  0.0879  0.2973 -0.0681  0.0105 -0.1998\n",
       "  0.1576 -0.3449 -0.2448 -0.1216  0.3219\n",
       "  0.2405  0.0922  0.0080 -0.3896  0.2327\n",
       "  0.2249 -0.2841  0.4381 -0.2988  0.3350\n",
       " -0.1683  0.2320 -0.0294  0.2497  0.0134\n",
       "  0.2130 -0.1540  0.3665  0.2317 -0.2806\n",
       "  0.3682  0.1374 -0.3794  0.1069  0.3271\n",
       " -0.3931  0.0649  0.2663 -0.1443  0.2581\n",
       "  0.2788  0.2402 -0.3745 -0.2952  0.2720\n",
       " -0.0728 -0.3068  0.3741  0.3533  0.4344\n",
       "  0.0398  0.1703 -0.4457  0.1875 -0.4108\n",
       " -0.0019 -0.1116 -0.3784  0.1674  0.0395\n",
       " -0.3549  0.1357  0.2084  0.2010  0.4211\n",
       " -0.3698 -0.2166 -0.2154 -0.1044 -0.1012\n",
       " [torch.FloatTensor of size 30x5], Parameter containing:\n",
       " -0.3112\n",
       "  0.4331\n",
       " -0.3731\n",
       " -0.3513\n",
       " -0.1493\n",
       " -0.4090\n",
       "  0.1979\n",
       " -0.0833\n",
       "  0.3295\n",
       "  0.4213\n",
       "  0.3244\n",
       " -0.4246\n",
       " -0.3646\n",
       " -0.1318\n",
       "  0.2508\n",
       "  0.4389\n",
       " -0.0385\n",
       " -0.3597\n",
       " -0.0996\n",
       "  0.3712\n",
       "  0.3626\n",
       "  0.3649\n",
       "  0.0138\n",
       " -0.2636\n",
       "  0.3017\n",
       " -0.1196\n",
       "  0.2771\n",
       " -0.3666\n",
       " -0.2848\n",
       " -0.3058\n",
       " [torch.FloatTensor of size 30], Parameter containing:\n",
       " \n",
       " Columns 0 to 9 \n",
       " -0.0192  0.0726 -0.1115  0.1696 -0.0846 -0.1622  0.1545 -0.0166 -0.0159  0.1112\n",
       " -0.0337  0.0588  0.1040 -0.0209 -0.0775  0.0303 -0.0529 -0.0590  0.1133  0.1095\n",
       " -0.0053  0.1800 -0.0147  0.1323  0.0143 -0.1342  0.1340  0.1546  0.1757 -0.0262\n",
       " \n",
       " Columns 10 to 19 \n",
       " -0.1643 -0.1069  0.0692 -0.0764  0.0498  0.0892  0.0859  0.0160  0.0422  0.0736\n",
       " -0.1488  0.1518  0.1026 -0.0352 -0.0094  0.0935  0.0470  0.0248 -0.1129 -0.0471\n",
       " -0.0838  0.0515  0.1278  0.1252  0.0925 -0.0533 -0.0073  0.0447  0.1246 -0.0309\n",
       " \n",
       " Columns 20 to 29 \n",
       " -0.1394 -0.1166  0.1810 -0.1098  0.1749  0.0807 -0.1797  0.1158  0.1400  0.1084\n",
       " -0.0962  0.1396 -0.1248  0.0421 -0.0182  0.1568 -0.1819  0.0764 -0.1746  0.0498\n",
       "  0.1539 -0.0037 -0.0863 -0.0428  0.1541 -0.0743 -0.0534  0.0394  0.0569 -0.1734\n",
       " [torch.FloatTensor of size 3x30], Parameter containing:\n",
       " -0.1428\n",
       " -0.1778\n",
       " -0.0326\n",
       " [torch.FloatTensor of size 3]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network(input_size=5, nb_action=3)\n",
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Deep Q Learning\n",
    "\n",
    "* References\n",
    "    - [http://pytorch.org/docs/master/optim.html#torch.optim.Adam]\n",
    "    - [https://arxiv.org/abs/1412.6980]\n",
    "    - [http://pytorch.org/docs/master/torch.html#torch.unsqueeze]\n",
    "    - [http://pytorch.org/docs/master/nn.html?highlight=softmax#torch.nn.Softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn(object):\n",
    "    \n",
    "    def __init__(self, input_size, nb_action, gamma):\n",
    "        '''\n",
    "        Initializing the Dqn object\n",
    "        \n",
    "        input_size = size of input vector\n",
    "        nb_action = size of output vector\n",
    "        gamma = the discount factor gamma in Q - learning\n",
    "        '''\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # the reward window to house the latest rewards and calculating rolling mean\n",
    "        self.reward_window = []\n",
    "        \n",
    "        # instance of Network class\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        \n",
    "        # instance of ReplayMemory class\n",
    "        self.memory = ReplayMemory(capacity=100000)\n",
    "        \n",
    "        # as defined here http://pytorch.org/docs/master/optim.html#torch.optim.Adam\n",
    "        self.optimizer = optim.Adam(params=self.model.parameters(), lr=0.001)\n",
    "        \n",
    "        # as defined here http://pytorch.org/docs/master/torch.html#torch.unsqueeze\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0.0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        # sureity of neural network on action it decides to play \n",
    "        temperature = 7\n",
    "        \n",
    "        # as defined here http://pytorch.org/docs/master/nn.html?highlight=softmax#torch.nn.Softmax\n",
    "        probs = F.softmax(self.model.forward(Variable(state, volatile=True)) * temperature)\n",
    "        action = probs.multinomial()\n",
    "        return action.data[0,0]\n",
    "    \n",
    "    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):\n",
    "        outputs = self.model(batch_state).gather(1, batch_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
